{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf18940-99b3-494d-bf05-2426dedf63b1",
   "metadata": {},
   "source": [
    "### Functions used by RDT_to_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55956cd3-32fc-4c65-ba30-cf57ded9dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, CurveFit\n",
    "using Dates, DataFrames, DSP\n",
    "using NativeFileDialog\n",
    "using Plots, Printf\n",
    "using Statistics\n",
    "\n",
    "# Function to apply polynomial fit to WSE's affected by GPS errors\n",
    "function fix_gps_errors(heave, date, gps_flag)   \n",
    "##############################################\n",
    "    \n",
    "    gps_errors = findall(x -> x == 1, gps_flag)\n",
    "    heave_length = length(heave)\n",
    "    \n",
    "    if !isempty(gps_errors)\n",
    "        \n",
    "        println(length(gps_errors), \" GPS errors at \", Dates.format.(date, \"yyyy-mm-dd HH:MM\"))\n",
    "        flush(stdout)\n",
    "        \n",
    "        for ii in reverse(gps_errors)\n",
    "\n",
    "            error_center = ii\n",
    "\n",
    "            if error_center <= 3\n",
    "                error_center = 3\n",
    "            end\n",
    "\n",
    "            if error_center >= heave_length - 3\n",
    "                error_center = heave_length - 3\n",
    "            end\n",
    "            \n",
    "            # User-selected offset either side of GPS error\n",
    "            lower_offset = upper_offset = 120\n",
    "\n",
    "            if error_center <= lower_offset\n",
    "                lower_offset = error_center - 1\n",
    "            end\n",
    "\n",
    "            if error_center + upper_offset > heave_length\n",
    "                upper_offset = heave_length - error_center\n",
    "            end\n",
    "\n",
    "            # Ensure there are at least 3 points for fitting\n",
    "            lower_offset = max(lower_offset, 2)\n",
    "            upper_offset = max(upper_offset, 2)\n",
    "    \n",
    "            # Handle edge cases\n",
    "            left_side_points = max(1, error_center - lower_offset):error_center\n",
    "            right_side_points = error_center:min(heave_length, error_center + upper_offset)\n",
    "\n",
    "            # Fit curve to subset of heave before GPS error\n",
    "            fit1 = curve_fit(Polynomial, left_side_points, heave[left_side_points], 2)\n",
    "            yfit1 = fit1.(left_side_points)\n",
    "            yfit1[end] = 0.0  # set the last point of the left fit to 0\n",
    "\n",
    "            # Fit curve to subset of heave after GPS error\n",
    "            fit2 = curve_fit(Polynomial, right_side_points, heave[right_side_points], 2)\n",
    "            yfit2 = fit2.(right_side_points)\n",
    "            yfit2[1] = 0.0  # set the first point of the right fit to 0\n",
    "\n",
    "            # Apply polynomial results to WSEs on both sides of GPS error\n",
    "            heave[left_side_points] .= heave[left_side_points] - yfit1\n",
    "            heave[right_side_points] .= heave[right_side_points] - yfit2\n",
    "            heave[ii] = 0.0  # set WSE at GPS error location to 0\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(heave)\n",
    "    \n",
    "end  # fix_gps_errors()\n",
    "\n",
    "\n",
    "# smooth the spectra into bands centered on 0.05Hz spacing (i.e. 0:0.005:0.64)\n",
    "function smooth_spectra(Pden_in, sample_frequency)\n",
    "##################################################\n",
    "\n",
    "    nyquist = sample_frequency/2\n",
    "\n",
    "    freq_in = range(0, stop=nyquist, length=length(Pden_in))\n",
    "\n",
    "    freq_out = [0.0]\n",
    "    Pden_smoothed = [mean(Pden_in[1:8])]\n",
    "\n",
    "    i = 9\n",
    "    while i <= length(Pden_in)\n",
    "\n",
    "        push!(freq_out,freq_in[i+8])\n",
    "\n",
    "        if i < length(Pden_in)-16\n",
    "\n",
    "            push!(Pden_smoothed, mean(Pden_in[i:i+16]))\n",
    "\n",
    "        end\n",
    "\n",
    "        i+=16\n",
    "\n",
    "    end\n",
    "\n",
    "    push!(Pden_smoothed, mean(Pden_in[end-8:end]))\n",
    "            \n",
    "    return(freq_out, Pden_smoothed)\n",
    "        \n",
    "end    # smooth_spectra()\n",
    "\n",
    "\n",
    "function calc_f2_Pden2(heave, sample_frequency)\n",
    "###############################################\n",
    "    \n",
    "    # Show spectra using Welch's method to better define bimodal events\n",
    "    ps_w = welch_pgram(Float64.(heave), 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning);\n",
    "    f2 = freq(ps_w);\n",
    "    Pden2 = power(ps_w)    \n",
    "\n",
    "    return(f2, Pden2)\n",
    "\n",
    "end    # calc_f2_Pden2()\n",
    "    \n",
    "\n",
    "# Function to read binary file\n",
    "function read_binary_file(filename)\n",
    "###################################\n",
    "    \n",
    "    open(filename, \"r\") do file\n",
    "        return(read(file))\n",
    "    end\n",
    "    \n",
    "end    # read_binary_file()\n",
    "\n",
    "\n",
    "# Function to find all header indices using list comprehension\n",
    "function find_headers(data, header)\n",
    "###################################\n",
    "    \n",
    "    header_length = length(header)\n",
    "    data_length = length(data)\n",
    "    [i for i in 1:(data_length - header_length + 1) if data[i:i+header_length-1] == header]\n",
    "    \n",
    "end    # find_headers()\n",
    "\n",
    "\n",
    "# Function to calculate the sample rate\n",
    "function get_sample_frequency(ii, RDT_data)\n",
    "######################################\n",
    "    \n",
    "    sample_frequency_hex = parse(UInt32, \"0x\" * string(RDT_data[ii+11], base=16, pad=2) * string(RDT_data[ii+12], base=16, pad=2) *\n",
    "                           string(RDT_data[ii+13], base=16, pad=2) * string(RDT_data[ii+14], base=16, pad=2))\n",
    "    reinterpret(Float32, sample_frequency_hex)\n",
    "    \n",
    "end    # get_sample_frequency()\n",
    "\n",
    "\n",
    "# apply coarse filter to displacements that are greater than 20\n",
    "function filter_large_values(arr)\n",
    "#################################\n",
    "    \n",
    "    return([abs(x) >= 20 ? 0.0 : x for x in arr])\n",
    "    \n",
    "end    # filter_large_values()\n",
    "\n",
    "\n",
    "# Function to parse records and update DataFrame\n",
    "function parse_record(RDT_data, ii, displacement_df, message_length, sample_frequency)\n",
    "########################################################################\n",
    "    \n",
    "    utc = try\n",
    "        yr = parse(Int, string(RDT_data[ii+5], base=16) * string(RDT_data[ii+6], base=16, pad=2), base=16)\n",
    "        month = parse(Int, string(RDT_data[ii+7], base=16, pad=2), base=16)\n",
    "        day = parse(Int, string(RDT_data[ii+8], base=16, pad=2), base=16)\n",
    "        hour = parse(Int, string(RDT_data[ii+9], base=16, pad=2), base=16)\n",
    "        minute = parse(Int, string(RDT_data[ii+10], base=16, pad=2), base=16)\n",
    "        DateTime(yr, month, day, hour, minute)\n",
    "    catch e\n",
    "        return(nothing)\n",
    "    end\n",
    "\n",
    "    heave = AbstractFloat[]; north = AbstractFloat[]; west = AbstractFloat[] \n",
    "    gps_flag = []\n",
    "\n",
    "    for jj in 15:6:message_length\n",
    "        \n",
    "        push!(heave,Float64(reinterpret(Int16, parse(UInt16, \"0x\" * string(RDT_data[ii+jj], base=16, pad=2) * \n",
    "            string(RDT_data[ii+jj+1], base=16, pad=2))) / 100))\n",
    "        north_hex = parse(UInt16, \"0x\" * string(RDT_data[ii+jj+2], base=16, pad=2) * string(RDT_data[ii+jj+3], base=16, pad=2))\n",
    "        push!(north,Float64(reinterpret(Int16, north_hex) / 100))\n",
    "        push!(west,Float64(reinterpret(Int16, parse(UInt16, \"0x\" * string(RDT_data[ii+jj+4], base=16, pad=2) * \n",
    "            string(RDT_data[ii+jj+5], base=16, pad=2))) / 100))\n",
    "        gps_error = parse(Int, last(string(north_hex, base=2, pad=16), 1))\n",
    "        push!(gps_flag, gps_error)\n",
    "\n",
    "    end\n",
    "\n",
    "    # Set heave value to 0 if its absolute value is greater than or equal to 20\n",
    "    global heave = filter_large_values(heave)\n",
    "    global north = filter_large_values(north)\n",
    "    global west = filter_large_values(west)\n",
    "    \n",
    "    if sum(gps_flag) > 0\n",
    "        heave = fix_gps_errors(heave, utc, gps_flag)\n",
    "    end\n",
    "\n",
    "    global f2, Pden2 = calc_f2_Pden2(heave, sample_frequency)\n",
    "\n",
    "    push!(displacement_df, (utc, heave, north, west, f2, Pden2))\n",
    "    \n",
    "end    # parse_record()\n",
    "\n",
    "\n",
    "function process_RDT_file(infil)\n",
    "################################    \n",
    "    RDT_data = read_binary_file(infil)\n",
    "    header = UInt8[0x2a, 0x30, 0x36, 0x0a, 0x26]\n",
    "    header_indices = find_headers(RDT_data, header)\n",
    "\n",
    "    reclen = 13840\n",
    "    good_records = findall(==(reclen), diff(header_indices))\n",
    "\n",
    "    displacement_df = DataFrame(Date = DateTime[], Heave = Vector{Float64}[], North = Vector{Float64}[], West = Vector{Float64}[], \n",
    "        f2 = Vector{Float64}[], Pden2 = Vector{Float64}[])\n",
    "\n",
    "    for ii in header_indices[good_records]\n",
    "        sample_frequency = get_sample_frequency(ii, RDT_data)\n",
    "        parse_record(RDT_data, ii, displacement_df, reclen, sample_frequency)\n",
    "    end\n",
    "\n",
    "    return(displacement_df)\n",
    "    \n",
    "end    # process_RDT_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f414b-3426-4443-9258-9adcf07e5e3f",
   "metadata": {},
   "source": [
    "### Read a single .RDT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998e919-fe15-43af-aa28-08a4b9ec6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "\n",
    "# Read the binary file\n",
    "infil = pick_file(\"F:\\\\Card Data\\\\\", filterlist=\"*RDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff3458-31d7-48ce-9294-76c01947d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "println(\"Selected \",infil)\n",
    "    \n",
    "displacement_df = process_RDT_file(infil)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ccdfc-bf81-415d-b2dd-68ff4f661ea0",
   "metadata": {},
   "source": [
    "### Read an entire directory of .RDT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35748c77-c5db-4e80-8b83-d853cf2ba2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Glob\n",
    "\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "\n",
    "# Main code to process all .RDT files in a selected directory\n",
    "directory_path = pick_folder()\n",
    "rdt_files = glob(\".//*.RDT\", directory_path)\n",
    "global rdt_files = rdt_files[1:end-1]  # remove the file named TMP.RDT\n",
    "\n",
    "@time begin\n",
    "    \n",
    "    println(\"Processing files in directory: \", directory_path)\n",
    "    \n",
    "    all_displacement_dfs = []\n",
    "\n",
    "    for infil in rdt_files\n",
    "        println(\"Processing \", infil)\n",
    "        displacement_df = process_RDT_file(infil)\n",
    "        push!(all_displacement_dfs, displacement_df)\n",
    "    end\n",
    "\n",
    "    # Combine all displacement DataFrames\n",
    "    displacement_df = vcat(all_displacement_dfs...)\n",
    "\n",
    "    all_displacement_dfs = nothing\n",
    "    \n",
    "\n",
    "    # sort the displacement_df on Date ascending\n",
    "    sort!(displacement_df, :Date)\n",
    "\n",
    "    # Remove rows where the year is less than 1990\n",
    "    filter!(row -> year(row.Date) >= 1990, displacement_df)\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a877a5-0179-4ee0-adec-43dc98b8aaf0",
   "metadata": {},
   "source": [
    "### Save Serialized output df to .gzip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d564e-24d2-4dd7-bf3b-10c086ea620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CodecZlib, Serialization\n",
    "\n",
    "# Serialize and compress the DataFrame to a file\n",
    "# Serialize the DataFrame to a file\n",
    "outfil = \".\\\\Data\\\\\" * split(directory_path,\"\\\\\")[end-1]*\"_\"*Dates.format(Date(Date(displacement_df.Date[1])), \"yyyy-mm-dd\")*\n",
    "    \"_to_\"*Dates.format(Date(Date(displacement_df.Date[end])), \"yyyy-mm-dd\")*\".bin\"\n",
    "\n",
    "open(outfil, \"w\") do io\n",
    "    gz = GzipCompressorStream(io)    # Create a Gzip compressor stream\n",
    "    serialize(gz, displacement_df)   # Serialize the DataFrame and write it to the compressed stream\n",
    "    close(gz)                        # Close the compressor stream to ensure all data is written\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c8518-a93d-44d1-bee6-9541f8811067",
   "metadata": {},
   "source": [
    "### Write the RDT data to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43a0f4-9110-4b79-a753-6ba72e8c4521",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using JSON\n",
    "\n",
    "JSon_file = \".\\\\Data\\\\\" * split(directory_path,\"\\\\\")[end-1]*\"_\"*Dates.format(Date(Date(displacement_df.Date[1])), \"yyyy-mm-dd\")*\n",
    "    \"_to_\"*Dates.format(Date(Date(displacement_df.Date[end])), \"yyyy-mm-dd\")*\".csv\"\n",
    "\n",
    "println(\"Writing JSON-formatted data to \",JSon_file)\n",
    "flush(stdout)\n",
    "\n",
    "@time begin\n",
    "\n",
    "    # Remove rows where the year is less than 1990\n",
    "    filter!(row -> year(row.Date) >= 1990, displacement_df)\n",
    "\n",
    "    # Convert array columns to JSON strings\n",
    "    json_displacement_df = copy(displacement_df)\n",
    "    \n",
    "    for col in names(displacement_df)\n",
    "        if col != :Date\n",
    "            json_displacement_df[!, col] = JSON.json.(displacement_df[!, col])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Write the JSON-encoded DataFrame to a CSV \n",
    "    CSV.write(JSon_file, json_displacement_df);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f1b98-4576-451a-b1e6-338c0a6af396",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "@time brgin\n",
    "    for i in 1:1000\n",
    "        print(i)\n",
    "    end\n",
    "end\n",
    "\n",
    "addprocs(4)  # Adjust the number of processors\n",
    "\n",
    "@time brgin\n",
    "    @distributed for i in 1:10\n",
    "    print(i)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77d0ca-9252-4aa1-842c-816c0094bead",
   "metadata": {},
   "source": [
    "### Plot spectra for each month in displacement_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a581ed68-4018-4f24-808f-8893329cb2ff",
   "metadata": {},
   "source": [
    "using DataFrames, Dates, Plots\n",
    "\n",
    "# Function to extract year and month from DateTime\n",
    "function year_month(date::DateTime)\n",
    "###################################\n",
    "    \n",
    "    return (year(date), month(date))\n",
    "    \n",
    "end    # year_month()\n",
    "\n",
    "\n",
    "# Function to check for significant low-frequency peaks\n",
    "function has_significant_low_frequency_peak(frequencies, spectra, threshold=0.05, peak_threshold=0.1)\n",
    "#####################################################################################################\n",
    "    \n",
    "    low_freq_indices = findall(f .< threshold for f in frequencies)\n",
    "    low_freq_spectra = spectra[low_freq_indices]\n",
    "    \n",
    "    if !isempty(low_freq_spectra) && maximum(low_freq_spectra) > peak_threshold\n",
    "        return true\n",
    "    end\n",
    "    \n",
    "    return(false)\n",
    "    \n",
    "end    # has_significant_low_frequency_peak()\n",
    "\n",
    "\n",
    "# Sort by highest low-frequency peak\n",
    "function sort_by_low_frequency_peak(displacement_df)\n",
    "#######################################\n",
    "    \n",
    "    low_freq_peaks = [maximum(displacement_df.Pden2[i][findall(f .< 0.05 for f in displacement_df.f2[i])]) for i in 1:nrow(displacement_df)]\n",
    "    sorted_indices = sortperm(low_freq_peaks, rev=true)\n",
    "    \n",
    "    return(displacement_df[sorted_indices, :])\n",
    "    \n",
    "end    # sort_by_low_frequency_peak()\n",
    "\n",
    "\n",
    "@time begin\n",
    "\n",
    "    # get the site name\n",
    "    site = split(directory_path,\"\\\\\")[end-1]\n",
    "\n",
    "    # Remove rows where the year is less than 1990\n",
    "    filter!(row -> year(row.Date) >= 1990, displacement_df)\n",
    "    \n",
    "    # Add a column for year and month\n",
    "    displacement_df[!, :YearMonth] = year_month.(displacement_df.Date)\n",
    "    \n",
    "    # Group by YearMonth\n",
    "    grouped_displacement_df = groupby(displacement_df, :YearMonth)\n",
    "    \n",
    "    # Plot the spectra for each month/year\n",
    "    for group in grouped_displacement_df\n",
    "        \n",
    "        year, month = group[1, :YearMonth]\n",
    "        plot_title = @sprintf(\"%s %04d-%02d\", site, year, month)\n",
    "        plot_file = \".\\\\Plots\\\\\"*replace(replace(plot_title, \"/\" => \"_\"), \" \" => \"_\")*\"_low_frequency.png\"\n",
    "     \n",
    "        # Create a new plot\n",
    "        p = plot()\n",
    "        \n",
    "        # Sort the group by highest low-frequency peak\n",
    "        sorted_group = sort_by_low_frequency_peak(group)\n",
    "        \n",
    "        # Identify the top 15 spectra with significant low-frequency peaks\n",
    "        labeled_count = 0\n",
    "        \n",
    "        for row in eachrow(sorted_group)\n",
    "            if has_significant_low_frequency_peak(row.f2, row.Pden2)\n",
    "                color = :red\n",
    "                label = (labeled_count < 15) ? string(row.Date) : \"\"\n",
    "                labeled_count += 1\n",
    "            else\n",
    "                color = :lightgray\n",
    "                label = \"\"\n",
    "            end\n",
    "            \n",
    "            p = plot!(row.f2, row.Pden2, label=label, color=color,)\n",
    "        end\n",
    "    \n",
    "        p1_plot = plot(p,size=(1800,600), framestyle = :box, fg_legend=:transparent, legend=:topright,\n",
    "            title=plot_title, xlabel=\"Frequency\", ylabel=\"Spectral Density\",\n",
    "            xticks=0:0.05:0.6, xtickfontsize=8, xlims=(0,0.4),\n",
    "            leftmargin = 15Plots.mm, grid=true, gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "        # Display the plot\n",
    "    \n",
    "        try\n",
    "                                                        \n",
    "            savefig(plot_file)\n",
    "            println(\"\\nPlot file saved as \"*plot_file)\n",
    "        \n",
    "        catch\n",
    "        \n",
    "            \"Alert: Plot not saved!\"\n",
    "        \n",
    "        end\n",
    "        \n",
    "        display(p1_plot)\n",
    "    end\n",
    "\n",
    "    grouped_displacement_df = nothing\n",
    "    displacement_df = nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa0974-6154-443c-900a-dd907698036c",
   "metadata": {},
   "source": [
    "### Write data to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca20cfb-2425-4fb0-a195-ddb8dbe13dca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Hdisplacement_df5, Serialization, Dates\n",
    "\n",
    "# Function to serialize a vector of arrays\n",
    "function serialize_column(col)\n",
    "    io = IOBuffer()\n",
    "    serialize(io, col)\n",
    "    return take!(io)\n",
    "end\n",
    "\n",
    "# Convert DateTime to Unix timestamp (in seconds)\n",
    "dates_unix = [Dates.datetime2unix(d) for d in displacement_df.Date]\n",
    "\n",
    "# Serialize the columns with arrays\n",
    "heave_serialized = serialize_column(displacement_df.Heave)\n",
    "north_serialized = serialize_column(displacement_df.North)\n",
    "west_serialized = serialize_column(displacement_df.West)\n",
    "f2_serialized = serialize_column(displacement_df.f2)\n",
    "pden2_serialized = serialize_column(displacement_df.Pden2)\n",
    "\n",
    "# Write to Hdisplacement_df5 file with chunked storage and compression\n",
    "h5open(\"displacement_df.h5\", \"w\") do file\n",
    "    dset = create_dataset(file, \"Date\", datatype(dates_unix), dataspace(dates_unix); chunk=length(dates_unix), compress=true, deflate=9)\n",
    "    write(dset, dates_unix)\n",
    "\n",
    "    dset = create_dataset(file, \"Heave\", datatype(heave_serialized), dataspace(heave_serialized); chunk=length(heave_serialized), compress=true, deflate=9)\n",
    "    write(dset, heave_serialized)\n",
    "\n",
    "    dset = create_dataset(file, \"North\", datatype(north_serialized), dataspace(north_serialized); chunk=length(north_serialized), compress=true, deflate=9)\n",
    "    write(dset, north_serialized)\n",
    "\n",
    "    dset = create_dataset(file, \"West\", datatype(west_serialized), dataspace(west_serialized); chunk=length(west_serialized), compress=true, deflate=9)\n",
    "    write(dset, west_serialized)\n",
    "\n",
    "    dset = create_dataset(file, \"f2\", datatype(f2_serialized), dataspace(f2_serialized); chunk=length(f2_serialized), compress=true, deflate=9)\n",
    "    write(dset, f2_serialized)\n",
    "\n",
    "    dset = create_dataset(file, \"Pden2\", datatype(pden2_serialized), dataspace(pden2_serialized); chunk=length(pden2_serialized), compress=true, deflate=9)\n",
    "    write(dset, pden2_serialized)\n",
    "end\n",
    "\n",
    "println(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b67a8-af41-45f1-b0ff-ddff81a35573",
   "metadata": {},
   "source": [
    "### Use Seralize to save df to .bin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5851e8-e449-412e-9eb9-53c6415845e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the DataFrame to a file\n",
    "outfil = \".\\\\Data\\\\\" * split(directory_path,\"\\\\\")[end-1]*\"_\"*Dates.format(Date(Date(displacement_df.Date[1])), \"yyyy-mm-dd\")*\n",
    "    \"_to_\"*Dates.format(Date(Date(displacement_displacement_df.Date[end])), \"yyyy-mm-dd\")*\".bin\"\n",
    "\n",
    "println(\"Writing binary-formatted data to \",outfil)\n",
    "flush(stdout)\n",
    "\n",
    "open(outfil, \"w\") do io\n",
    "    serialize(io, displacement_df)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5dfd1-5313-43b7-8971-5ac5a05b1226",
   "metadata": {},
   "source": [
    "### Use Seralize to read gzipped .bin file to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8abd6-984c-45dc-8584-b0f8bd1ef9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CodecZlib, Serialization, DataFrames\n",
    "using NativeFileDialog\n",
    "\n",
    "function read_gzip_file(io)\n",
    "    \n",
    "    gz = GzipDecompressorStream(io)                # Create a Gzip decompressor stream\n",
    "    deserialized_displacement_df = deserialize(gz) # Deserialize the DataFrame from the decompressed stream\n",
    "    close(gz)                                      # Close the decompressor stream\n",
    "    \n",
    "    return(deserialized_displacement_df)\n",
    "    \n",
    "end    # read_gzip_file()\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "# Select the binary file\n",
    "infil = pick_file(pwd()*\"\\\\Data\\\\\", filterlist=\"*bin\")\n",
    "\n",
    "println(\"Selected \", infil)\n",
    "\n",
    "@time begin\n",
    "    # Deserialize the DataFrame from the file\n",
    "    displacement_df2 = open(read_gzip_file, infil, \"r\")\n",
    "end\n",
    "\n",
    "# Verify the contents\n",
    "println(displacement_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1d57-4fa5-48e0-b367-16465a944564",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = displacement_df2.Date\n",
    "\n",
    "using Dates, DSP\n",
    "using NativeFileDialog, Plots\n",
    "using Tk\n",
    "\n",
    "function plot_long_wave(start_date,tt,heave)\n",
    "############################################\n",
    "    \n",
    "    start_tt = tt[1]; last_tt=tt[end]\n",
    "    \n",
    "##    responsetype = Lowpass(0.04)\n",
    "    responsetype = Bandpass(.01, .04)\n",
    "    designmethod = Butterworth(4)\n",
    "    long_heave = filt(digitalfilter(responsetype, designmethod), heave);\n",
    "    \n",
    "    tm_tick = range(first(tt),last(tt),step=Minute(5))\n",
    "    ticks = Dates.format.(tm_tick,\"MM:SS\")\n",
    "    \n",
    "    p1 = plot(tt, heave, label=\"\")\n",
    "    p1 = plot!(tt, long_heave, lw=:3, lc=:red, label=\"Long waves > 25s\")\n",
    "    \n",
    "    plot1 = plot(p1, \n",
    "            xlabel=\"Time\", xlim=(start_tt,last_tt), xticks=(tm_tick,ticks), xtickfontsize=7,\n",
    "            ylabel=\"Heave (m)\", tickfontsize=8, \n",
    "            title=file_choice[1]*\" Long waves\", framestyle = :box,\n",
    "            leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(1200, 600), colorbar=false,    \n",
    "        gridlinewidth=0.5, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    display(plot1)\n",
    "\n",
    "end    # plot_long_wave()\n",
    "\n",
    "\n",
    "dates_array = Dates.format.(dates, \"yyyy-mm-ddTHH:MM:SS\")\n",
    "\n",
    "w = Toplevel(\"Select Start Date\", 235, 650)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, dates_array)\n",
    "\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1,  expand=true, fill=\"both\")\n",
    "\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Tk.Button(f, \"Ok\")\n",
    "pack(b)\n",
    "\n",
    "println(\"Select a time from the menu!\")\n",
    "flush(stdout)\n",
    "\n",
    "bind(b, \"command\") do path\n",
    "                    \n",
    "    get_value(lb);\n",
    "\n",
    "    global file_choice = get_value(lb);\n",
    "    global start_date = DateTime(file_choice[1])\n",
    "    \n",
    "    index = findfirst(==(start_date), dates)\n",
    "    heave = displacement_df2.Heave[index]\n",
    "    ll = length(heave)\n",
    "    tt = displacement_df2.Date[index] .+ Microsecond.(ceil.((0:ll-1) * 1000000))\n",
    "    plot_long_wave(start_date,tt,heave)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28ebed-9083-4336-bb71-1c438edc9e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21fe97-dda8-493d-a2f2-77fcf2f9b23e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using Wavelets, Plots\n",
    "\n",
    "index = findfirst(==(start_date), dates)\n",
    "\n",
    "ll = length(displacement_df2.Heave[index])\n",
    "tt = displacement_df2.Date[index] .+ Microsecond.(ceil.((0:ll-1) * 1000000))\n",
    "heave = displacement_df2.Heave[index]\n",
    "\n",
    "# Perform the continuous wavelet transform using the Morlet wavelet\n",
    "wavelet_coeffs = cwt(heave, wavelet(WT.morl))\n",
    "\n",
    "# Get the dimensions of the wavelet coefficients matrix\n",
    "num_scales, num_times = size(wavelet_coeffs)\n",
    "\n",
    "# Generate a time vector (assuming a sampling rate of 1 for simplicity)\n",
    "time = 1:num_times\n",
    "\n",
    "# Generate a scale vector (for visualization purposes, these are just scale indices)\n",
    "scales = 1:num_scales\n",
    "\n",
    "# Plot the scalogram (magnitude of the wavelet coefficients)\n",
    "heatmap(time, scales, abs.(wavelet_coeffs),\n",
    "    xlabel = \"Time\",\n",
    "    ylabel = \"Scale\",\n",
    "    title = \"Continuous Wavelet Transform\",\n",
    "    color = :viridis)\n",
    "\n",
    "# Display the plot\n",
    "plot!(size=(1000,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32d426-af2e-494f-92aa-8758b8ad62c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, HDF5, Dates\n",
    "\n",
    "# Flatten nested arrays and store dimensions\n",
    "flattened_heave = vcat(displacement_df.Heave...)\n",
    "heave_dims = Int32[length(arr) for arr in displacement_df.Heave]\n",
    "\n",
    "flattened_north = vcat(displacement_df.North...)\n",
    "north_dims = Int32[length(arr) for arr in displacement_df.North]\n",
    "\n",
    "flattened_west = vcat(displacement_df.West...)\n",
    "west_dims = Int32[length(arr) for arr in displacement_df.West]\n",
    "\n",
    "flattened_f2 = vcat(displacement_df.f2...)\n",
    "f2_dims = Int32[length(arr) for arr in displacement_df.f2]\n",
    "\n",
    "flattened_pden2 = vcat(displacement_df.Pden2...)\n",
    "pden2_dims = Int32[length(arr) for arr in displacement_df.Pden2]\n",
    "\n",
    "dates_unix = [Dates.datetime2unix(d) for d in displacement_df.Date]\n",
    "\n",
    "# Write data to Hdisplacement_df5\n",
    "h5write(\"dataframe2.h5\", \"Date\", dates_unix)\n",
    "h5write(\"dataframe2.h5\", \"Heave\", flattened_heave)\n",
    "h5write(\"dataframe2.h5\", \"Heave_dims\", heave_dims)\n",
    "h5write(\"dataframe2.h5\", \"North\", flattened_north)\n",
    "h5write(\"dataframe2.h5\", \"North_dims\", north_dims)\n",
    "h5write(\"dataframe2.h5\", \"West\", flattened_west)\n",
    "h5write(\"dataframe2.h5\", \"West_dims\", west_dims)\n",
    "h5write(\"dataframe2.h5\", \"f2\", flattened_f2)\n",
    "h5write(\"dataframe2.h5\", \"f2_dims\", f2_dims)\n",
    "h5write(\"dataframe2.h5\", \"Pden2\", flattened_pden2)\n",
    "h5write(\"dataframe2.h5\", \"Pden2_dims\", pden2_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799c0ff-f810-4752-925e-74f329f2c6b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, HDF5, Dates\n",
    "\n",
    "# Read data from HDF5\n",
    "dates_unix = h5read(\"dataframe2.h5\", \"Date\")\n",
    "flattened_heave = h5read(\"dataframe2.h5\", \"Heave\")\n",
    "heave_dims = h5read(\"dataframe2.h5\", \"Heave_dims\")\n",
    "\n",
    "flattened_north = h5read(\"dataframe2.h5\", \"North\")\n",
    "north_dims = h5read(\"dataframe2.h5\", \"North_dims\")\n",
    "\n",
    "flattened_west = h5read(\"dataframe2.h5\", \"West\")\n",
    "west_dims = h5read(\"dataframe2.h5\", \"West_dims\")\n",
    "\n",
    "flattened_f2 = h5read(\"dataframe2.h5\", \"f2\")\n",
    "f2_dims = h5read(\"dataframe2.h5\", \"f2_dims\")\n",
    "\n",
    "flattened_pden2 = h5read(\"dataframe2.h5\", \"Pden2\")\n",
    "pden2_dims = h5read(\"dataframe2.h5\", \"Pden2_dims\")\n",
    "\n",
    "# Reconstruct nested arrays\n",
    "function reconstruct_nested_array(flattened, dims)\n",
    "    arrs = []\n",
    "    start_idx = 1\n",
    "    for dim in dims\n",
    "        end_idx = start_idx + dim - 1\n",
    "        push!(arrs, flattened[start_idx:end_idx])\n",
    "        start_idx = end_idx + 1\n",
    "    end\n",
    "    return arrs\n",
    "end\n",
    "\n",
    "heave = reconstruct_nested_array(flattened_heave, heave_dims)\n",
    "north = reconstruct_nested_array(flattened_north, north_dims)\n",
    "west = reconstruct_nested_array(flattened_west, west_dims)\n",
    "f2 = reconstruct_nested_array(flattened_f2, f2_dims)\n",
    "pden2 = reconstruct_nested_array(flattened_pden2, pden2_dims)\n",
    "\n",
    "dates = [Dates.unix2datetime(unix) for unix in dates_unix]\n",
    "\n",
    "df2 = DataFrame(Date=dates, Heave=heave, North=north, West=west, f2=f2, Pden2=pden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ae0fb-63ab-4bb0-b93b-54de476113df",
   "metadata": {},
   "source": [
    "### Read data from Hdisplacement_df5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d7b1d-0abb-452d-adb8-47a79ff21d63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using DataFrames, Hdisplacement_df5, Serialization, Dates\n",
    "\n",
    "# Function to deserialize a vector of arrays\n",
    "function deserialize_column(data)\n",
    "    io = IOBuffer(data)\n",
    "    return deserialize(io)\n",
    "end\n",
    "\n",
    "# Read from Hdisplacement_df5 file\n",
    "dates_unix = h5read(\"displacement_df.h5\", \"Date\")\n",
    "heave_serialized = h5read(\"displacement_df.h5\", \"Heave\")\n",
    "north_serialized = h5read(\"displacement_df.h5\", \"North\")\n",
    "west_serialized = h5read(\"displacement_df.h5\", \"West\")\n",
    "f2_serialized = h5read(\"displacement_df.h5\", \"f2\")\n",
    "pden2_serialized = h5read(\"displacement_df.h5\", \"Pden2\")\n",
    "\n",
    "# Convert Unix timestamp back to DateTime\n",
    "dates = [Dates.unix2datetime(d) for d in dates_unix]\n",
    "\n",
    "# Deserialize the columns\n",
    "heave = deserialize_column(heave_serialized)\n",
    "north = deserialize_column(north_serialized)\n",
    "west = deserialize_column(west_serialized)\n",
    "f2 = deserialize_column(f2_serialized)\n",
    "pden2 = deserialize_column(pden2_serialized)\n",
    "\n",
    "# Reconstruct DataFrame\n",
    "displacement_df = DataFrame(Date = dates, Heave = heave, North = north, West = west, f2 = f2, Pden2 = pden2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76118624-6fdc-409b-a438-701acaf62ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
